# Overview
This repository contains a large-scale C++ project developed as part of the **Optimization** course during the fifth semester at **AGH University of Krakow**. The project is a collaborative group assignment, where we worked in groups of three. The goal was to implement various optimization methods within a given framework.

---

[Image](media)

# Authors

- **[Arek](https://github.com/arekan144)**: Implementation of optimization algorithms, test exercises for algorithm validation, and the contributions to spreadsheet for results.
- **[Filip](https://github.com/Filip-Rak)**: Implementation of the real-world problem exercises, focusing on practical applications of optimization methods, as well as contributions to the spreadsheet for results and graphs.
- **[Paulina](https://github.com/paulina-grab-owska)**: Responsible for compiling the lengthy coursework report, documenting the optimization process, methodology, and results of the project in detail.
  
- **dr. inż. Łukasz Sztangret**: Provided the foundational framework and guidance for the project.

This project was a collaborative effort, with each team member contributing their unique expertise to ensure that the solution was both comprehensive and effective.

---

# Optimization Methods

- **Expansion Method** - A technique used to expand and explore regions in optimization problems, typically used in iterative algorithms to find optimal solutions.

- **Hooke-Jeeves Method**: A direct search optimization method, also known as the **pattern search method**, used to find optimal solutions without requiring derivatives.

- **Rosenbrock Method**: Known as **Rosenbrock's valley**, this method is used in optimization problems where the objective function has a narrow, curved valley. It's commonly used for testing optimization algorithms.

- **Nelder-Mead Simplex Method** - Also known as the **simplex method**, is a popular approach for finding the minimum of a function in a multidimensional space. It is a heuristic method that doesn’t require derivatives.

- **Steepest Descent Method** - An optimization technique where the search proceeds in the direction of the steepest negative gradient, aiming to find the local minimum of a function.

- **Conjugate Gradient Method** - An iterative optimization technique used to solve large linear systems and optimization problems, particularly when the objective function is quadratic.

- **Newton's Method** - A root-finding algorithm that uses the derivative of a function to iteratively find better approximations of its roots, often used for optimization in multivariable functions.

- **Powell's Method** - A conjugate direction method for multidimensional optimization that doesn’t require derivatives and is effective for functions that are not smooth or continuous.

- **Evolutionary Algorithm – (μ+λ) Strategy** - A form of evolutionary computation where a population of candidate solutions is evolved over generations using selection, recombination, and mutation to find an optimal solution.

---

# Framework 

---

# Repository Structure

---

# Installtion

---
